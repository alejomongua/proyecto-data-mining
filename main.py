# -*- coding: utf-8 -*-
"""proyecto_data_mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wRCrdGcEskWQ5hvh2U3qHztHgYf96jb0

#Universidad Nacional de Colombia

##Minería de Datos

Luis Alejandro Mongua López - lamongual@unal.edu.co

Ivanhoe Rozo Rojas - irozor@unal.edu.co

Camilo Alfonso Mosquera Benavides - camosquerab@unal.edu.co

## **Proyecto de data mining**

_Septiembre 2022_

##Descripción general del dataset
El dataset corresponde a los resultados de las revisiones técnico-mecánicas de varias topologías de vehículos en CDAs (Centros de Diagnóstico Automotor) en todo el país entre los años 2014 y 2022, incluye algunos valores cualitativos y otros cuantitativos. La cantidad de registros se estima en 900.000, organizados en 34 variables.

Es de anotar que algunos datos no aplican para algunos tipos de vehículo, por ejemplo, las motocicletas no cuentan con valores cuantitativos del estado de la suspensión, y los vehículos que funcionan con combustible diesel no tienen valores en los campos de emisiones de HC, CO, CO2 ni O2 (sus emisiones se miden en porcentaje de opacidad).

Los datos fueron extraídos de la base de datos de inspecciones de varios CDA y fueron anonimizados para no violar las políticas de privacidad y protección de datos personales.

Primero importamos las librerías que vamos a usar
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelBinarizer

from google.colab import drive
drive.mount('/content/drive')

"""Definimos la constante que contiene las columnas que nos interesan para este análisis"""

CAMPOS_GASES = [
    'gases__ralenti__co',
    'gases__ralenti__co2',
    'gases__ralenti__hc',
    'gases__ralenti__o2',
    'modelo',
    'marca',
    'linea',
    'kilometraje',
    'codigo_resultado',
    'secuencia',
    'fecha_fin',
]

"""## Integracion
El dataset es recolectado de los diferentes bases de datos integrando los datos encontrados en documentos csv de diferentes revisiones de los distintos CDA; con el dataset integrado se carga en el drive para tener acceso a el.

Se carga el dataset
"""

df = pd.read_hdf('/content/drive/Shareddrives/Data_mining/dataset.hdf')
df

"""Se filtran los registros con los que vamos a trabajar:

* tipo de revisión 1, que corresponde a automóviles livianos.
* Tipo de Combustible: gasolina

Se extraen las columnas que nos interesan
"""

carros_df = df[(df['tipo_revision_id'] == 1) & (df['combustible'] == 'GASOLINA')][CAMPOS_GASES]

carros_df.shape

"""## Tratamiento de valores perdidos

Actualmente se tiene un dataset de las siguientes dimensiones:
"""

carros_df.shape

"""Como el dataset es relativamente extenso, podemos eliminar los registros que tengan valores faltantes"""

carros_df.dropna(how='any', inplace=True)

"""Después de esto, tenemos un dataset aún lo suficientemente grande

"""

carros_df.shape

"""## Eliminar duplicados

Se eliminan en el dataset los registros duplicados, sin embargo, para esto no tenemos en cuenta las columnas _kilometraje_, *codigo_resultado* y _secuencia_, ya que la principal razón para encontrar valores repetidos es debido a que los vehículos, cuando su prueba de emisiones de gases es aprobada pero tienen algún defecto en otro item, deben presentarse nuevamente en el CDA pero la prueba de gases no se realiza nuevamente, por lo que los datos de la inspección anterior se conservan.
"""

def eliminar_duplicados(dataframe, campos_ignorados):
    """Se eliminan los registros duplicados"""
    # No se tienen en cuenta algunos campos
    columnas_para_duplicados = list(dataframe.columns)
    for campo in campos_ignorados:
        columnas_para_duplicados.remove(campo)

    dataframe.drop_duplicates(inplace=True, subset=columnas_para_duplicados)

eliminar_duplicados(carros_df, ['kilometraje', 'codigo_resultado', 'secuencia'])

"""# Muestreo

Sacar el promedio en el conjunto completo de los datos es una función muy demorada. Por lo tanto lo que hacemos es sacar un promedio de una muestra, en este caso, del 10 % de los datos. La computación es mucho más rápida y el resultado es muy parecido al promedio del conjunto de datos completo.

Lo mismo aplica para la desviación estándar
"""

sample_df = carros_df.sample(frac=0.1)

mean = sample_df._get_numeric_data().mean()
stdev = sample_df._get_numeric_data().std()

"""## Detección de outliers

Se eliminan los valores que sabemos que no tienen sentido. Los carros inspeccionados son carros usados, por lo tanto no tienen sentido kilometrajes demasiado bajos:

"""

def eliminar_valores_incoherentes(dataframe, campos):
    """
    Se eliminan los valores que, desde el conocimiento de los datos
    se sabe que corresponden a errores en la lectura del dato o en la
    digitación
    Esta función recibe como parámetros el dataframe y un diccionario
    de campos donde cada llave tiene asociado un array de longitud 2
    donde se almacena el límite inferior y el límite superior respectivamente
    de los valores que se consideran coherentes
    """
    for campo in campos:
        niveles = campos[campo]
        lim_inferior = niveles[0]
        lim_superior = niveles[1]

        dataframe = dataframe[(dataframe[campo] > lim_inferior) & (dataframe[campo] < lim_superior)]
    
    return dataframe

carros_df = eliminar_valores_incoherentes(carros_df, {
  'gases__ralenti__co': [-0.5, 25],
  'gases__ralenti__co2': [-0.5, 25],
  'gases__ralenti__hc': [-0.5, 2**16],
  'gases__ralenti__o2': [-0.5, 25],
  'kilometraje': [101, 10**6],
  'secuencia': [0, 3]
})
carros_df.shape

"""Ahora podemos intentar detectar los outliers con base en el algoritmo de K vecinos cercanos 

https://towardsdatascience.com/k-nearest-neighbors-knn-for-anomaly-detection-fdf8ee160d13
"""

CAMPOS_DE_INTERES = [
    'gases__ralenti__co',
    'gases__ralenti__co2',
    'gases__ralenti__o2',
    'gases__ralenti__hc',
]

# Se realiza normalización por z_score y se extraen los valores que nos 
# interesan
X = ((carros_df[CAMPOS_DE_INTERES] - mean) / stdev)[CAMPOS_DE_INTERES].values

# Instanciar modelo
nbrs = NearestNeighbors(n_neighbors = 100)

# Se entrena el modelo
nbrs.fit(X)

# Se calcula la distancia a los 100 vecinos más cercanos para cada medición
distances, indexes = nbrs.kneighbors(X)

# Se calcula la media de las 100 distancias para cada punto
mean_distance = distances.mean(axis=1)

# Se grafican los resultados
plt.plot(mean_distance)
plt.show()

"""Creamos un dataset de distancias para analizar estos datos"""

distancias_df = pd.DataFrame({'distancia': mean_distance}, index=carros_df.index)

distancias_df.describe()

"""Podemos apreciar que hay una fuerte tendencia de los datos hacia el cero. Podemos intentar un valor de umbral de detección de outliers en una distancia de 1"""

carros_df = carros_df[distancias_df['distancia'] <= 1]
carros_df.shape

"""## Agregación

Se va a realizar la agrupacion por linea de carros con la intencion de observar el promedio de emision de gases contaminantes de los vehiculos automotores, con el fin de intentar encontrar las líneas de vehículos que tienen las emisiones contaminantes más bajas 
"""

# Se calcula el tamaño de cada grupo
grouped_size = carros_df.groupby(['marca', 'linea']).size()

# Se calcula el promedio de cada grupo
grouped_mean = carros_df.groupby(['marca', 'linea']).mean()

# Se crea un nuevo dataset
agregado = pd.concat([grouped_size, grouped_mean], axis=1, sort=False)

# Se seleccionan los registros con más de 100 datos
agregado = agregado[grouped_size > 100]

# Se ordena por promedio de CO para hacernos una idea de las líneas que menos contaminan
agregado = agregado.sort_values(by=['gases__ralenti__co'])

# Mostramos el dataset
agregado.head(20)

"""# Discretización

Se crea un atributo "artificial" llamado _antiguedad_ para almacenar la edad del vehículo en el momento de la prueba
"""

carros_df['antiguedad'] = carros_df['fecha_fin'].map(lambda x: x.year) - carros_df['modelo']

# Se eliminan las dos columnas ahora innecesarias: fecha_fin y modelo
carros_df.drop(['modelo', 'fecha_fin'], axis=1, inplace=True)

"""A continuación se va a discretizar las variables de _antigüedad_ y _kilometraje_, que darán una indicación del uso que ha tenido el vehículo

Primero dibujaremos histogramas para hacernos una idea de la distrubución de los datos
"""

plt.hist(carros_df['antiguedad'], bins=40)
plt.show()

plt.hist(carros_df['kilometraje'], bins=40)
plt.show()

"""Usaremos una discretización de frecuencia constante usando la función qcut"""

uso, bins = pd.qcut(carros_df['kilometraje'], 5, labels=[
    'muy poco uso',
    'poco uso',
    'uso normal',
    'mucho uso',
    'muy alto uso',
], retbins=True)

# Renombrar
uso.rename(index='uso', inplace=True)

# Imprima los intervalos
for i in range(1, len(bins)):
  print(f'Intervalo {i}: [{bins[i - 1]} - {bins[i]})')

# Eliminamos la columna original
carros_df.drop('kilometraje', axis= 1, inplace=True)

plt.hist(uso)
plt.show()

"""Ahora se discretizará la antigüedad:"""

edad, bins = pd.qcut(carros_df['antiguedad'], 5, labels=[
    'muy nuevo',
    'nuevo',
    'normal',
    'antiguo',
    'muy antiguo',
], retbins=True)

# Renombrar
edad.rename(index='edad', inplace=True)

# Imprima los intervalos
for i in range(1, len(bins)):
  print(f'Intervalo {i}: [{bins[i - 1]} - {bins[i]})')

# Eliminamos la columna original
carros_df.drop('antiguedad', axis= 1, inplace=True)

plt.hist(edad)
plt.show()

# Únalo con el resto del dataset
carros_df = pd.concat([carros_df, uso, edad], axis=1)
carros_df

"""## Binarización

Es posible binarizar las columnas que poseen valores nominales. En este caso, binarizaremos las columnas codigo_resultado y secuencia para poder tener columnas que describan mejor los campos que estos valores representan
"""

columns = ['codigo_resultado',	'secuencia']
data = carros_df[columns]

# Binarizamos las columnas
for column in columns:
  one_hot = pd.get_dummies(data[column], prefix=column)
  carros_df.drop(column, axis=1, inplace=True)
  carros_df = carros_df.join(one_hot)

# Renombramos las columas para que describan mejor su contenido
carros_df.rename(columns={
      'codigo_resultado_1': 'aprobado',
      'codigo_resultado_2': 'reprobado',
      'secuencia_1': 'primera_inspeccion',
      'secuencia_2': 'reinspeccion',
    },
    inplace=True)

carros_df

"""## Reducción de dimensionalidad

Conocemos de antemano que los resultados de la medición de gases están correlacionados, porque son producto de un mismo proceso de combustión, así que podemos intentar reducir la dimencionalidad de estos cuatro valores en 2 componentes principales para de esta manera poder dibujar los datos
"""

# Definir las características que, para el caso de estudio, corresponden a las emisiones
features = ['gases__ralenti__co',	'gases__ralenti__co2', 'gases__ralenti__hc',	'gases__ralenti__o2']

# Separar las características 'features'
x = carros_df.loc[:, features].values

# Separar las características 'features'
y = carros_df.loc[:,['marca']].values

# Standardizing the features
x = StandardScaler().fit_transform(x)

x.shape

pca = PCA(n_components=2) # Se definen componentes
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['pc1', 'pc2'])
finalDf = pd.concat([principalDf, df[['marca']]], axis = 1)

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
marca = ['MAZDA', 'CHEVROLET','KIA', 'VOLKSWAGEN', 'HYUNDAI', 'LAND ROVER']
colors = ['r', 'g', 'b','y', 'm', 'k'] #Definir colores
for target, color in zip(marca,colors):
    indicesToKeep = finalDf['marca'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'pc1']
               , finalDf.loc[indicesToKeep, 'pc2']
                , c = color
               , s = 1)
ax.legend(marca)
ax.grid()